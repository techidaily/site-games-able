---
title: "Costly Graphics Card Choice: My NVIDIA Regret"
date: 2024-08-31T19:27:06.852Z
updated: 2024-09-01T19:27:06.852Z
tags:
  - games
categories:
  - games
description: "This Article Describes Costly Graphics Card Choice: My NVIDIA Regret"
excerpt: "This Article Describes Costly Graphics Card Choice: My NVIDIA Regret"
keywords: GPU Cost Analysis,Nvidia Overpayment,Graphic Card Expense,High-End GPU Pricing,Graphics Purchase Reflection,NVIDIA Regretful Buy,Expensive GPU Selection
thumbnail: https://thmb.techidaily.com/9b9d2e5e3221cfe29f11fdae3e34a8712951d1bbc94b5d263ea270cbf9e85714.jpg
---

## Costly Graphics Card Choice: My NVIDIA Regret

### Quick Links

* [My GPU Usage Is Far From 100 Percent](#my-gpu-usage-is-far-from-100-percent)
* [Power Consumption Is Very High](#power-consumption-is-very-high)
* [Too Expensive for Diminishing Returns](#too-expensive-for-diminishing-returns)

### Key Takeaways

* The games I normally play don't utilize my RTX 4090 fully, and my CPU holds its performance back at 1440p resolution.
* At full power, the RTX 4090 can easily draw over 450W, which isn't ideal if you want to keep your electricity bills in check.
* Consider the RTX 4080 Super if you don't plan to play games at 4K resolution. It's far less power hungry with a 320W TDP.

 Despite launching in 2022, the RTX 4090 is still the world's fastest graphics card as of early 2024\. Although I enjoyed using it to play AAA games over the past year and a half, I still regret buying it for a few reasons.

<!-- affiliate ads begin -->
<a href="https://store.nero.com/order/checkout.php?PRODS=42296685&QTY=1&AFFILIATE=108875&CART=1"><img src="http://cdnwww.nero.com/nero-com-wAssets/img/banners/2022/video-pp/ScreenshotSlider/Nero-Video-Advanced-editing.JPG" border="0">Simple and intuitive video editing
ðŸŽ¬ Nero Video:
The powerful video editing program for your Windows PC</a>
<!-- affiliate ads end -->
## 1 My GPU Usage Is Far From 100 Percent

 My biggest issue since upgrading to the RTX 4090 is that my graphics card is often not fully utilized. And that's due to a couple of factors.

 Firstly, the games that I usually play are more CPU-intensive rather than GPU-intensive. Valorant, Call of Duty: Warzone, and Fortnite are some of the games that I play regularly.

 Secondly, I rarely play games at 4K resolution. Note that at lower resolutions, games put more stress on the CPU rather than the GPU to push more frames. To get more GPU usage in these situations, you need the fastest CPU you can possibly get.

 I have a triple-monitor setup where the primary monitor is a 1440p 360Hz panel. I only use my 4K 160Hz monitor to play single-player AAA titles like Assassin's Creed Mirage and Avatar: Frontiers of Pandora.

 Therefore, in games like Valorant, my GPU usage is not even close to 100 percent. Take a look at the screenshot below, where my RTX 4090 is pushing nearly 500FPS at 35% GPU usage.

![RTSS OSD stats appearing in Valorant](https://static1.makeuseofimages.com/wordpress/wp-content/uploads/2024/04/rtss-osd-stats-appearing-in-valorant.png)

<!-- affiliate ads begin -->
<a href="https://secure.2checkout.com/order/checkout.php?PRODS=11224199&QTY=1&AFFILIATE=108875&CART=1"><img src="https://secure.avangate.com/images/merchant/e09fdffe648a30658a9657bbed7b2388/products/copy_boxshot_lyricvideo.png" border="0">Lyric Video Creator Professional Version</a>
<!-- affiliate ads end -->
 Even in graphically demanding AAA games, my GPU usage rarely exceeds 90 percent. And that's because my CPU can't keep up. I use a Ryzen 5900X, which is a generation older than the top-of-the-line AMD Ryzen processors available today, like[the Ryzen 7800X3D, 7900X3D, and 7950X3D](https://www.makeuseof.com/ryzen-9-7950x3d-vs-ryzen-9-7900x3d-vs-ryzen-7-7800x3d/) .

<!-- affiliate ads begin -->
<a href="https://secure.2checkout.com/order/checkout.php?PRODS=174416&QTY=1&AFFILIATE=108875&CART=1"><img src="https://www.easygifanimator.net/images/gif-animator.png" border="0">Easy GIF Animator is a powerful animated GIF editor and the top tool for creating animated pictures, banners, buttons and GIF videos. You get extensive animation editing features, animation effects, unmatched image quality and optimization for the web. No other GIF animation software matches our features and ease of use, that's why Easy GIF Animator is so popular.</a>
<!-- affiliate ads end -->
## 2 Power Consumption Is Very High

 The RTX 4090 is a monster of a GPU that consumes a lot of power. The Founders Edition (reference) version of the RTX 4090 has a[TDP (Thermal Design Power)](https://www.makeuseof.com/tag/thermal-design-power-technology-explained/) of 450W. This means the GPU can consume up to 450W under general usage.

 However, the RTX 4090 I own is no ordinary unit. It's the Colorful iGame RTX 4090 Neptune, a liquid-cooled variant with a 360mm radiator that has a TDP of 630W! Like this one, several RTX 4090 variants from NVIDIA's board partners have a TDP higher than 450W.

 When I play Cyberpunk 2077 at 4K resolution with maxed-out settings, the GPU easily consumes 500W, which is way too much for someone who just plays games. In comparison, the RTX 4080 Super, NVIDIA's second-best offering, has a 320W TDP, which is a lot more manageable for someone looking to keep their electricity bills in check.

![a triple-monitor PC gaming setup with Cyberpunk 2077 running on the main display](https://static1.makeuseofimages.com/wordpress/wp-content/uploads/wm/2024/04/a-triple-monitor-pc-gaming-setup-with-cyberpunk-2077-running-on-the-main-display.jpg)

<!-- affiliate ads begin -->
<a href="https://turtlebeachus.sjv.io/c/5597632/1988416/23719" target="_top" id="1988416"><img src="//a.impactradius-go.com/display-ad/23719-1988416" border="0" alt="" width="600" height="600"/></a><img height="0" width="0" src="https://imp.pxf.io/i/5597632/1988416/23719" style="position:absolute;visibility:hidden;" border="0" />
<!-- affiliate ads end -->
 Hamlin Rozario/[MakeUseOf](https://www.makeuseof.com/author/hamlin-rozario/)

 That said, it's worth noting that power draw isn't a major issue when I'm playing less GPU-demanding games like Valorant or Fortnite because the GPU usage is typically low.

<!-- affiliate ads begin -->
<a href="https://martinic.evyy.net/c/5597632/1422856/4482" target="_top" id="1422856"><img src="//a.impactradius-go.com/display-ad/4482-1422856" border="0" alt="" width="580" height="309"/></a>
<!-- affiliate ads end -->
## 3 Too Expensive for Diminishing Returns

 Lastly, it comes down to the money I spent on this graphics card. NVIDIA launched the RTX 4090 with an MSRP of $1,599, which is already way too expensive for a graphics card.

 However, the Colorful Neptune RTX 4090 I have cost me a little over two grand, but at that time, I thought it was worth it because of the overclocking headroom I got from liquid cooling. Another reason is that this card isn't nearly as chunky as other air-cooled RTX 4090 cards on the market.

 Unfortunately, I haven't even come close to maximizing the RTX 4090's full potential because of the kind of games I mostly play. I now feel like I could've spent half the money on an RTX 4080 for similar frame rates on esports titles like Valorant, Overwatch 2, or Fortnite. To get the most out of my RTX 4090 now, I need one of[the fastest gaming CPUs from AMD or Intel](https://www.makeuseof.com/amd-vs-intel-gaming/) .

 So, if you're on the fence about buying an RTX 4090, think about the games you play and the resolution you play at because I don't want you to make the same mistake as I did. I wouldn't recommend buying an RTX 4090 if you don't plan to play AAA games at 4K resolution regularly.


<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-7571918770474297"
     data-ad-slot="1223367746"></ins>



<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7571918770474297"
     data-ad-slot="8358498916"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>


